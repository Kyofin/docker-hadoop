apiVersion: v1
kind: ConfigMap
metadata:
  name: bigdata-hadoop
  labels:
    app: hadoop
data:
  bootstrap.sh: |
    #!/bin/bash

    : ${HADOOP_PREFIX:=/usr/local/hadoop}

    . $HADOOP_PREFIX/etc/hadoop/hadoop-env.sh

    # Directory to find config artifacts
    CONFIG_DIR="/tmp/hadoop-config"

    # create hadoop temp dir
     mkdir -p /root/hadoop/tmp

    # Copy config files from volume mount

    for f in slaves core-site.xml hdfs-site.xml mapred-site.xml yarn-site.xml httpfs-site.xml yarn-env.sh; do
      if [[ -e ${CONFIG_DIR}/$f ]]; then
        cp ${CONFIG_DIR}/$f $HADOOP_PREFIX/etc/hadoop/$f
      else
        echo "ERROR: Could not find $f in $CONFIG_DIR"
        exit 1
      fi
    done

    # installing libraries if any - (resource urls added comma separated to the ACP system variable)
    cd $HADOOP_PREFIX/share/hadoop/common ; for cp in ${ACP//,/ }; do  echo == $cp; curl -LO $cp ; done; cd -

    if [[ "${HOSTNAME}" =~ "hdfs-nn" ]]; then

      if [ -f /root/hdfs/namenode/current/VERSION ]
      then
        echo "==============The file exist , namenode no need format ==============="
      else
        echo "==============The file doesn't exist , namenode  need format ==============="
        mkdir -p /root/hdfs/namenode
        $HADOOP_PREFIX/bin/hdfs namenode -format -force -nonInteractive
      fi

      $HADOOP_PREFIX/sbin/hadoop-daemon.sh start namenode
    fi

    if [[ "${HOSTNAME}" =~ "hdfs-dn" ]]; then
      mkdir -p /root/hdfs/datanode

      #  wait up to 30 seconds for namenode
      (while [[ $count -lt 15 && -z `curl -sf http://bigdata-hadoop-hdfs-nn:50070` ]]; do ((count=count+1)) ; echo "Waiting for bigdata-hadoop-hdfs-nn" ; sleep 2; done && [[ $count -lt 15 ]])
      [[ $? -ne 0 ]] && echo "Timeout waiting for hdfs-nn, exiting." && exit 1

      $HADOOP_PREFIX/sbin/hadoop-daemon.sh start datanode
    fi

    if [[ "${HOSTNAME}" =~ "yarn-rm" ]]; then
      cp ${CONFIG_DIR}/start-yarn-rm.sh $HADOOP_PREFIX/sbin/
      cd $HADOOP_PREFIX/sbin
      chmod +x start-yarn-rm.sh
      ./start-yarn-rm.sh
    fi
    
     if [[ "${HOSTNAME}" =~ "mr-jobhistory" ]]; then
      cp ${CONFIG_DIR}/start-mr-jobhistory.sh $HADOOP_PREFIX/sbin/
      cd $HADOOP_PREFIX/sbin
      mkdir -p $HADOOP_PREFIX/logs
      chmod +x start-mr-jobhistory.sh
      ./start-mr-jobhistory.sh
    fi

    if [[ "${HOSTNAME}" =~ "yarn-nm" ]]; then
      sed -i '/<\/configuration>/d' $HADOOP_PREFIX/etc/hadoop/yarn-site.xml
      cat >> $HADOOP_PREFIX/etc/hadoop/yarn-site.xml <<- EOM
      <property>
        <name>yarn.nodemanager.resource.memory-mb</name>
        <value>${MY_MEM_LIMIT:-2048}</value>
      </property>

      <property>
        <name>yarn.nodemanager.resource.cpu-vcores</name>
        <value>${MY_CPU_LIMIT:-2}</value>
      </property>
    EOM
      echo '</configuration>' >> $HADOOP_PREFIX/etc/hadoop/yarn-site.xml
      cp ${CONFIG_DIR}/start-yarn-nm.sh $HADOOP_PREFIX/sbin/
      cd $HADOOP_PREFIX/sbin
      chmod +x start-yarn-nm.sh

      #  wait up to 30 seconds for resourcemanager
      (while [[ $count -lt 15 && -z `curl -sf http://bigdata-hadoop-yarn-rm:8088/ws/v1/cluster/info` ]]; do ((count=count+1)) ; echo "Waiting for bigdata-hadoop-yarn-rm" ; sleep 2; done && [[ $count -lt 15 ]])
      [[ $? -ne 0 ]] && echo "Timeout waiting for yarn-rm, exiting." && exit 1

      ./start-yarn-nm.sh
    fi

    if [[ $1 == "-d" ]]; then
      until find ${HADOOP_PREFIX}/logs -mmin -1 | egrep -q '.*'; echo "`date`: Waiting for logs..." ; do sleep 2 ; done
      tail -F ${HADOOP_PREFIX}/logs/* &
      while true; do sleep 1000; done
    fi

    if [[ $1 == "-bash" ]]; then
      /bin/bash
    fi

  core-site.xml: |
    <?xml version="1.0"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
    <configuration>
      <property>
            <name>fs.defaultFS</name>
            <value>jfs://myjfs</value>
        </property>
       <property>
          <name>fs.jfs.impl</name>
          <value>io.juicefs.JuiceFileSystem</value>
      </property>
      <property>
          <name>fs.AbstractFileSystem.jfs.impl</name>
          <value>io.juicefs.JuiceFS</value>
      </property>
      <property>
          <name>juicefs.meta</name>
          <value>mysql://root:bigdata@(10.81.17.8:33066)/juicefs_k8s</value>
      </property>
      <property>
          <name>juicefs.cache-size</name>
          <value>1024</value>
      </property>
      <property>
          <name>juicefs.access-log</name>
          <value>/tmp/juicefs.access.log</value>
      </property>
      <property>
          <name>juicefs.cache-dir</name>
          <value>/tmp/data/juicefs</value>
      </property>
            <property>
              <name>hadoop.tmp.dir</name>
              <value>file:///root/hadoop/tmp</value>
          </property>
        <!--        增加监控推送-->
        <!--     <property>
           <name>juicefs.push-gateway</name>
           <value>prometheus-pushgateway:9091</value>
         </property> 
         -->
  
        </configuration>

  hdfs-site.xml: |
    <?xml version="1.0"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
    <configuration>

      </configuration>

  mapred-site.xml: |
    <?xml version="1.0"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

    <configuration>
      <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
      </property>
      <property>
        <name>mapreduce.jobhistory.address</name>
        <value>bigdata-hadoop-mr-jobhistory-0.bigdata-hadoop-mr-jobhistory.bigdata-dev.svc.cluster.local:10020</value>
      </property>
      <property>
        <name>mapreduce.jobhistory.webapp.address</name>
        <value>bigdata-hadoop-mr-jobhistory-0.bigdata-hadoop-mr-jobhistory.bigdata-dev.svc.cluster.local:19888</value>
      </property>
     <property>
           <name>mapreduce.map.memory.mb</name>
           <value>5120</value>
     </property>
     <property>
           <name>mapreduce.reduce.memory.mb</name>
           <value>5120</value>
     </property>
    </configuration>

  slaves: |
    localhost

  start-yarn-nm.sh: |
    #!/usr/bin/env bash

    # Start all yarn daemons.  Run this on master node.

    echo "starting yarn daemons"

    bin=`dirname "${BASH_SOURCE-$0}"`
    bin=`cd "$bin"; pwd`

    DEFAULT_LIBEXEC_DIR="$bin"/../libexec
    HADOOP_LIBEXEC_DIR=${HADOOP_LIBEXEC_DIR:-$DEFAULT_LIBEXEC_DIR}
    . $HADOOP_LIBEXEC_DIR/yarn-config.sh

    # start resourceManager
    # "$bin"/yarn-daemon.sh --config $YARN_CONF_DIR  start resourcemanager
    # start nodeManager
    "$bin"/yarn-daemon.sh --config $YARN_CONF_DIR  start nodemanager
    # start proxyserver
    #"$bin"/yarn-daemon.sh --config $YARN_CONF_DIR  start proxyserver

  start-mr-jobhistory.sh: |
    #!/usr/bin/env bash

    # Start all yarn daemons.  Run this on master node.

    echo "starting mr jobhistory "

    bin=`dirname "${BASH_SOURCE-$0}"`
    bin=`cd "$bin"; pwd`

    DEFAULT_LIBEXEC_DIR="$bin"/../libexec
    HADOOP_LIBEXEC_DIR=${HADOOP_LIBEXEC_DIR:-$DEFAULT_LIBEXEC_DIR}
    . $HADOOP_LIBEXEC_DIR/yarn-config.sh

    # start resourceManager
    # "$bin"/yarn-daemon.sh --config $YARN_CONF_DIR  start resourcemanager
    # start nodeManager
    #"$bin"/yarn-daemon.sh --config $YARN_CONF_DIR  start nodemanager
    # start proxyserver
    #"$bin"/yarn-daemon.sh --config $YARN_CONF_DIR  start proxyserver
    # start mr history
    "$bin"/mr-jobhistory-daemon.sh --config $YARN_CONF_DIR  start historyserver


  start-yarn-rm.sh: |
    #!/usr/bin/env bash

    # Start all yarn daemons.  Run this on master node.

    echo "starting yarn daemons"

    bin=`dirname "${BASH_SOURCE-$0}"`
    bin=`cd "$bin"; pwd`

    DEFAULT_LIBEXEC_DIR="$bin"/../libexec
    HADOOP_LIBEXEC_DIR=${HADOOP_LIBEXEC_DIR:-$DEFAULT_LIBEXEC_DIR}
    . $HADOOP_LIBEXEC_DIR/yarn-config.sh

    # start resourceManager
    "$bin"/yarn-daemon.sh --config $YARN_CONF_DIR  start resourcemanager
    # start nodeManager
    # "$bin"/yarn-daemons.sh --config $YARN_CONF_DIR  start nodemanager
    # start proxyserver
    "$bin"/yarn-daemon.sh --config $YARN_CONF_DIR  start proxyserver

  httpfs-site.xml: |
    <configuration>

    </configuration>

  yarn-env.sh: |
    # User for YARN daemons
    export HADOOP_YARN_USER=${HADOOP_YARN_USER:-yarn}
    
    # resolve links - $0 may be a softlink
    export YARN_CONF_DIR="${YARN_CONF_DIR:-$HADOOP_YARN_HOME/conf}"
    
    # some Java parameters
    # export JAVA_HOME=/home/y/libexec/jdk1.6.0/
    if [ "$JAVA_HOME" != "" ]; then
      #echo "run java in $JAVA_HOME"
      JAVA_HOME=$JAVA_HOME
    fi
    
    if [ "$JAVA_HOME" = "" ]; then
      echo "Error: JAVA_HOME is not set."
      exit 1
    fi
    
    JAVA=$JAVA_HOME/bin/java
    JAVA_HEAP_MAX=-Xmx1000m 
    
    # For setting YARN specific HEAP sizes please use this
    # Parameter and set appropriately
    # YARN_HEAPSIZE=1000
    
    # check envvars which might override default args
    if [ "$YARN_HEAPSIZE" != "" ]; then
      JAVA_HEAP_MAX="-Xmx""$YARN_HEAPSIZE""m"
    fi
    
    # Resource Manager specific parameters
    
    # Specify the max Heapsize for the ResourceManager using a numerical value
    # in the scale of MB. For example, to specify an jvm option of -Xmx1000m, set
    # the value to 1000.
    # This value will be overridden by an Xmx setting specified in either YARN_OPTS
    # and/or YARN_RESOURCEMANAGER_OPTS.
    # If not specified, the default value will be picked from either YARN_HEAPMAX
    # or JAVA_HEAP_MAX with YARN_HEAPMAX as the preferred option of the two.
    #export YARN_RESOURCEMANAGER_HEAPSIZE=1000
    
    # Specify the max Heapsize for the timeline server using a numerical value
    # in the scale of MB. For example, to specify an jvm option of -Xmx1000m, set
    # the value to 1000.
    # This value will be overridden by an Xmx setting specified in either YARN_OPTS
    # and/or YARN_TIMELINESERVER_OPTS.
    # If not specified, the default value will be picked from either YARN_HEAPMAX
    # or JAVA_HEAP_MAX with YARN_HEAPMAX as the preferred option of the two.
    #export YARN_TIMELINESERVER_HEAPSIZE=1000
    
    # Specify the JVM options to be used when starting the ResourceManager.
    # These options will be appended to the options specified as YARN_OPTS
    # and therefore may override any similar flags set in YARN_OPTS
    #export YARN_RESOURCEMANAGER_OPTS=
    
    # Node Manager specific parameters
    
    # Specify the max Heapsize for the NodeManager using a numerical value
    # in the scale of MB. For example, to specify an jvm option of -Xmx1000m, set
    # the value to 1000.
    # This value will be overridden by an Xmx setting specified in either YARN_OPTS
    # and/or YARN_NODEMANAGER_OPTS.
    # If not specified, the default value will be picked from either YARN_HEAPMAX
    # or JAVA_HEAP_MAX with YARN_HEAPMAX as the preferred option of the two.
    #export YARN_NODEMANAGER_HEAPSIZE=1000
    
    # Specify the JVM options to be used when starting the NodeManager.
    # These options will be appended to the options specified as YARN_OPTS
    # and therefore may override any similar flags set in YARN_OPTS
    #export YARN_NODEMANAGER_OPTS=
    
    # so that filenames w/ spaces are handled correctly in loops below
    IFS=
    
    
    # default log directory & file
    if [ "$YARN_LOG_DIR" = "" ]; then
      YARN_LOG_DIR="$HADOOP_YARN_HOME/logs"
    fi
    if [ "$YARN_LOGFILE" = "" ]; then
      YARN_LOGFILE='yarn.log'
    fi
    
    # default policy file for service-level authorization
    if [ "$YARN_POLICYFILE" = "" ]; then
      YARN_POLICYFILE="hadoop-policy.xml"
    fi
    
    # restore ordinary behaviour
    unset IFS
    
    
    YARN_OPTS="$YARN_OPTS -Dhadoop.log.dir=$YARN_LOG_DIR"
    YARN_OPTS="$YARN_OPTS -Dyarn.log.dir=$YARN_LOG_DIR"
    YARN_OPTS="$YARN_OPTS -Dhadoop.log.file=$YARN_LOGFILE"
    YARN_OPTS="$YARN_OPTS -Dyarn.log.file=$YARN_LOGFILE"
    YARN_OPTS="$YARN_OPTS -Dyarn.home.dir=$YARN_COMMON_HOME"
    YARN_OPTS="$YARN_OPTS -Dyarn.id.str=$YARN_IDENT_STRING"
    YARN_OPTS="$YARN_OPTS -Dhadoop.root.logger=${YARN_ROOT_LOGGER:-INFO,console}"
    YARN_OPTS="$YARN_OPTS -Dyarn.root.logger=${YARN_ROOT_LOGGER:-INFO,console}"
    if [ "x$JAVA_LIBRARY_PATH" != "x" ]; then
      YARN_OPTS="$YARN_OPTS -Djava.library.path=$JAVA_LIBRARY_PATH"
    fi  
    YARN_OPTS="$YARN_OPTS -Dyarn.policy.file=$YARN_POLICYFILE"
    
    # 添加jmx监控开放
    export YARN_NODEMANAGER_OPTS="$YARN_NODEMANAGER_OPTS -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.port=8001"
    export YARN_RESOURCEMANAGER_OPTS="$YARN_RESOURCEMANAGER_OPTS -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.port=8002"

    

    

  yarn-site.xml: |
    <?xml version="1.0"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

    <configuration>
      <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>bigdata-hadoop-yarn-rm</value>
      </property>

      <!-- Bind to all interfaces -->
      <property>
        <name>yarn.resourcemanager.bind-host</name>
        <value>0.0.0.0</value>
      </property>
      <property>
        <name>yarn.nodemanager.bind-host</name>
        <value>0.0.0.0</value>
      </property>
      <property>
        <name>yarn.timeline-service.bind-host</name>
        <value>0.0.0.0</value>
      </property>
      <!-- /Bind to all interfaces -->

      <property>
        <name>yarn.nodemanager.vmem-check-enabled</name>
        <value>false</value>
      </property>

      <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
      </property>

      <property>
        <name>yarn.nodemanager.aux-services.mapreduce_shuffle.class</name>
        <value>org.apache.hadoop.mapred.ShuffleHandler</value>
      </property>

      <property>
        <description>List of directories to store localized files in.</description>
        <name>yarn.nodemanager.local-dirs</name>
        <value>/var/lib/hadoop-yarn/cache/${user.name}/nm-local-dir</value>
      </property>

      <property>
        <description>Where to store container logs.</description>
        <name>yarn.nodemanager.log-dirs</name>
        <value>/var/log/hadoop-yarn/containers</value>
      </property>

      <property>
        <description>Where to aggregate logs to.</description>
        <name>yarn.nodemanager.remote-app-log-dir</name>
        <value>/var/log/hadoop-yarn/apps</value>
      </property>
        
        <!--指定yarn.log.server.url所在节点，不开启的话，已完成的任务无法正常查看日志-->
        <property>
        <name>yarn.log.server.url</name>
        <value>http://bigdata-hadoop-mr-jobhistory-0.bigdata-hadoop-mr-jobhistory.bigdata-local.svc.cluster.local:19888/jobhistory/logs</value>
        </property>

      <property>
        <name>yarn.application.classpath</name>
        <value>
          /usr/local/hadoop/etc/hadoop,
          /usr/local/hadoop/share/hadoop/common/*,
          /usr/local/hadoop/share/hadoop/common/lib/*,
          /usr/local/hadoop/share/hadoop/hdfs/*,
          /usr/local/hadoop/share/hadoop/hdfs/lib/*,
          /usr/local/hadoop/share/hadoop/mapreduce/*,
          /usr/local/hadoop/share/hadoop/mapreduce/lib/*,
          /usr/local/hadoop/share/hadoop/yarn/*,
          /usr/local/hadoop/share/hadoop/yarn/lib/*
        </value>
      </property>
    </configuration>
