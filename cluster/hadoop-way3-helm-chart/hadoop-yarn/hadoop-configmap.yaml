apiVersion: v1
kind: ConfigMap
metadata:
  name: bigdata-hadoop
  labels:
    app: hadoop
data:
  bootstrap.sh: |
    #!/bin/bash

    : ${HADOOP_PREFIX:=/usr/local/hadoop}

    . $HADOOP_PREFIX/etc/hadoop/hadoop-env.sh

    # Directory to find config artifacts
    CONFIG_DIR="/tmp/hadoop-config"

    # create hadoop temp dir
     mkdir -p /root/hadoop/tmp

    # Copy config files from volume mount

    for f in slaves core-site.xml hdfs-site.xml mapred-site.xml yarn-site.xml httpfs-site.xml; do
      if [[ -e ${CONFIG_DIR}/$f ]]; then
        cp ${CONFIG_DIR}/$f $HADOOP_PREFIX/etc/hadoop/$f
      else
        echo "ERROR: Could not find $f in $CONFIG_DIR"
        exit 1
      fi
    done

    # installing libraries if any - (resource urls added comma separated to the ACP system variable)
    cd $HADOOP_PREFIX/share/hadoop/common ; for cp in ${ACP//,/ }; do  echo == $cp; curl -LO $cp ; done; cd -

    if [[ "${HOSTNAME}" =~ "hdfs-nn" ]]; then

      if [ -f /root/hdfs/namenode/current/VERSION ]
      then
        echo "==============The file exist , namenode no need format ==============="
      else
        echo "==============The file doesn't exist , namenode  need format ==============="
        mkdir -p /root/hdfs/namenode
        $HADOOP_PREFIX/bin/hdfs namenode -format -force -nonInteractive
      fi

      $HADOOP_PREFIX/sbin/hadoop-daemon.sh start namenode
    fi

    if [[ "${HOSTNAME}" =~ "hdfs-dn" ]]; then
      mkdir -p /root/hdfs/datanode

      #  wait up to 30 seconds for namenode
      (while [[ $count -lt 15 && -z `curl -sf http://bigdata-hadoop-hdfs-nn:50070` ]]; do ((count=count+1)) ; echo "Waiting for bigdata-hadoop-hdfs-nn" ; sleep 2; done && [[ $count -lt 15 ]])
      [[ $? -ne 0 ]] && echo "Timeout waiting for hdfs-nn, exiting." && exit 1

      $HADOOP_PREFIX/sbin/hadoop-daemon.sh start datanode
    fi

    if [[ "${HOSTNAME}" =~ "yarn-rm" ]]; then
      cp ${CONFIG_DIR}/start-yarn-rm.sh $HADOOP_PREFIX/sbin/
      cd $HADOOP_PREFIX/sbin
      chmod +x start-yarn-rm.sh
      ./start-yarn-rm.sh
    fi
    
     if [[ "${HOSTNAME}" =~ "mr-jobhistory" ]]; then
      cp ${CONFIG_DIR}/start-mr-jobhistory.sh $HADOOP_PREFIX/sbin/
      cd $HADOOP_PREFIX/sbin
      mkdir -p $HADOOP_PREFIX/logs
      chmod +x start-mr-jobhistory.sh
      ./start-mr-jobhistory.sh
    fi

    if [[ "${HOSTNAME}" =~ "yarn-nm" ]]; then
      sed -i '/<\/configuration>/d' $HADOOP_PREFIX/etc/hadoop/yarn-site.xml
      cat >> $HADOOP_PREFIX/etc/hadoop/yarn-site.xml <<- EOM
      <property>
        <name>yarn.nodemanager.resource.memory-mb</name>
        <value>${MY_MEM_LIMIT:-2048}</value>
      </property>

      <property>
        <name>yarn.nodemanager.resource.cpu-vcores</name>
        <value>${MY_CPU_LIMIT:-2}</value>
      </property>
    EOM
      echo '</configuration>' >> $HADOOP_PREFIX/etc/hadoop/yarn-site.xml
      cp ${CONFIG_DIR}/start-yarn-nm.sh $HADOOP_PREFIX/sbin/
      cd $HADOOP_PREFIX/sbin
      chmod +x start-yarn-nm.sh

      #  wait up to 30 seconds for resourcemanager
      (while [[ $count -lt 15 && -z `curl -sf http://bigdata-hadoop-yarn-rm:8088/ws/v1/cluster/info` ]]; do ((count=count+1)) ; echo "Waiting for bigdata-hadoop-yarn-rm" ; sleep 2; done && [[ $count -lt 15 ]])
      [[ $? -ne 0 ]] && echo "Timeout waiting for yarn-rm, exiting." && exit 1

      ./start-yarn-nm.sh
    fi

    if [[ $1 == "-d" ]]; then
      until find ${HADOOP_PREFIX}/logs -mmin -1 | egrep -q '.*'; echo "`date`: Waiting for logs..." ; do sleep 2 ; done
      tail -F ${HADOOP_PREFIX}/logs/* &
      while true; do sleep 1000; done
    fi

    if [[ $1 == "-bash" ]]; then
      /bin/bash
    fi

  core-site.xml: |
    <?xml version="1.0"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
    <configuration>
      <property>
            <name>fs.defaultFS</name>
            <value>jfs://myjfs</value>
        </property>
       <property>
          <name>fs.jfs.impl</name>
          <value>io.juicefs.JuiceFileSystem</value>
      </property>
      <property>
          <name>fs.AbstractFileSystem.jfs.impl</name>
          <value>io.juicefs.JuiceFS</value>
      </property>
      <property>
          <name>juicefs.meta</name>
          <value>mysql://root:eWJmP7yvpccHCtmVb61Gxl2XLzIrRgmT@(mysql-01:3306)/juicefs2_meta_minio</value>
      </property>
      <property>
          <name>juicefs.cache-size</name>
          <value>1024</value>
      </property>
      <property>
          <name>juicefs.access-log</name>
          <value>/tmp/juicefs.access.log</value>
      </property>
      <property>
          <name>juicefs.cache-dir</name>
          <value>/tmp/data/juicefs</value>
      </property>
            <property>
              <name>hadoop.tmp.dir</name>
              <value>file:///root/hadoop/tmp</value>
          </property>
        <!--        增加监控推送-->
        <!--     <property>
           <name>juicefs.push-gateway</name>
           <value>prometheus-pushgateway:9091</value>
         </property> 
         -->
  
        </configuration>

  hdfs-site.xml: |
    <?xml version="1.0"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
    <configuration>

      </configuration>

  mapred-site.xml: |
    <?xml version="1.0"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

    <configuration>
      <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
      </property>
      <property>
        <name>mapreduce.jobhistory.address</name>
        <value>bigdata-hadoop-mr-jobhistory-0.bigdata-hadoop-mr-jobhistory.bigdata-local.svc.cluster.local:10020</value>
      </property>
      <property>
        <name>mapreduce.jobhistory.webapp.address</name>
        <value>bigdata-hadoop-mr-jobhistory-0.bigdata-hadoop-mr-jobhistory.bigdata-local.svc.cluster.local:19888</value>
      </property>
     <property>
           <name>mapreduce.map.memory.mb</name>
           <value>5120</value>
     </property>
     <property>
           <name>mapreduce.reduce.memory.mb</name>
           <value>5120</value>
     </property>
    </configuration>

  slaves: |
    localhost

  start-yarn-nm.sh: |
    #!/usr/bin/env bash

    # Start all yarn daemons.  Run this on master node.

    echo "starting yarn daemons"

    bin=`dirname "${BASH_SOURCE-$0}"`
    bin=`cd "$bin"; pwd`

    DEFAULT_LIBEXEC_DIR="$bin"/../libexec
    HADOOP_LIBEXEC_DIR=${HADOOP_LIBEXEC_DIR:-$DEFAULT_LIBEXEC_DIR}
    . $HADOOP_LIBEXEC_DIR/yarn-config.sh

    # start resourceManager
    # "$bin"/yarn-daemon.sh --config $YARN_CONF_DIR  start resourcemanager
    # start nodeManager
    "$bin"/yarn-daemon.sh --config $YARN_CONF_DIR  start nodemanager
    # start proxyserver
    #"$bin"/yarn-daemon.sh --config $YARN_CONF_DIR  start proxyserver

  start-mr-jobhistory.sh: |
    #!/usr/bin/env bash

    # Start all yarn daemons.  Run this on master node.

    echo "starting mr jobhistory "

    bin=`dirname "${BASH_SOURCE-$0}"`
    bin=`cd "$bin"; pwd`

    DEFAULT_LIBEXEC_DIR="$bin"/../libexec
    HADOOP_LIBEXEC_DIR=${HADOOP_LIBEXEC_DIR:-$DEFAULT_LIBEXEC_DIR}
    . $HADOOP_LIBEXEC_DIR/yarn-config.sh

    # start resourceManager
    # "$bin"/yarn-daemon.sh --config $YARN_CONF_DIR  start resourcemanager
    # start nodeManager
    #"$bin"/yarn-daemon.sh --config $YARN_CONF_DIR  start nodemanager
    # start proxyserver
    #"$bin"/yarn-daemon.sh --config $YARN_CONF_DIR  start proxyserver
    # start mr history
    "$bin"/mr-jobhistory-daemon.sh --config $YARN_CONF_DIR  start historyserver


  start-yarn-rm.sh: |
    #!/usr/bin/env bash

    # Start all yarn daemons.  Run this on master node.

    echo "starting yarn daemons"

    bin=`dirname "${BASH_SOURCE-$0}"`
    bin=`cd "$bin"; pwd`

    DEFAULT_LIBEXEC_DIR="$bin"/../libexec
    HADOOP_LIBEXEC_DIR=${HADOOP_LIBEXEC_DIR:-$DEFAULT_LIBEXEC_DIR}
    . $HADOOP_LIBEXEC_DIR/yarn-config.sh

    # start resourceManager
    "$bin"/yarn-daemon.sh --config $YARN_CONF_DIR  start resourcemanager
    # start nodeManager
    # "$bin"/yarn-daemons.sh --config $YARN_CONF_DIR  start nodemanager
    # start proxyserver
    "$bin"/yarn-daemon.sh --config $YARN_CONF_DIR  start proxyserver

  httpfs-site.xml: |
    <configuration>

    </configuration>

  yarn-site.xml: |
    <?xml version="1.0"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

    <configuration>
      <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>bigdata-hadoop-yarn-rm</value>
      </property>

      <!-- Bind to all interfaces -->
      <property>
        <name>yarn.resourcemanager.bind-host</name>
        <value>0.0.0.0</value>
      </property>
      <property>
        <name>yarn.nodemanager.bind-host</name>
        <value>0.0.0.0</value>
      </property>
      <property>
        <name>yarn.timeline-service.bind-host</name>
        <value>0.0.0.0</value>
      </property>
      <!-- /Bind to all interfaces -->

      <property>
        <name>yarn.nodemanager.vmem-check-enabled</name>
        <value>false</value>
      </property>

      <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
      </property>

      <property>
        <name>yarn.nodemanager.aux-services.mapreduce_shuffle.class</name>
        <value>org.apache.hadoop.mapred.ShuffleHandler</value>
      </property>

      <property>
        <description>List of directories to store localized files in.</description>
        <name>yarn.nodemanager.local-dirs</name>
        <value>/var/lib/hadoop-yarn/cache/${user.name}/nm-local-dir</value>
      </property>

      <property>
        <description>Where to store container logs.</description>
        <name>yarn.nodemanager.log-dirs</name>
        <value>/var/log/hadoop-yarn/containers</value>
      </property>

      <property>
        <description>Where to aggregate logs to.</description>
        <name>yarn.nodemanager.remote-app-log-dir</name>
        <value>/var/log/hadoop-yarn/apps</value>
      </property>
        
        <!--指定yarn.log.server.url所在节点，不开启的话，已完成的任务无法正常查看日志-->
        <property>
        <name>yarn.log.server.url</name>
        <value>http://bigdata-hadoop-mr-jobhistory-0.bigdata-hadoop-mr-jobhistory.bigdata-local.svc.cluster.local:19888/jobhistory/logs</value>
        </property>

      <property>
        <name>yarn.application.classpath</name>
        <value>
          /usr/local/hadoop/etc/hadoop,
          /usr/local/hadoop/share/hadoop/common/*,
          /usr/local/hadoop/share/hadoop/common/lib/*,
          /usr/local/hadoop/share/hadoop/hdfs/*,
          /usr/local/hadoop/share/hadoop/hdfs/lib/*,
          /usr/local/hadoop/share/hadoop/mapreduce/*,
          /usr/local/hadoop/share/hadoop/mapreduce/lib/*,
          /usr/local/hadoop/share/hadoop/yarn/*,
          /usr/local/hadoop/share/hadoop/yarn/lib/*
        </value>
      </property>
    </configuration>
