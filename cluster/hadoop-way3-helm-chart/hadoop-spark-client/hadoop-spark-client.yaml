apiVersion: v1
kind: Pod
metadata:
  name: hadoop-spark-client
  labels:
    app:  hadoop-spark-client
spec:
  containers:
    - name: client
      image: registry.mufankong.top/bigdata/hadoop-spark-client:h2.7.2-s3.2.1
      imagePullPolicy: IfNotPresent
      env:
        - name: TZ
          value: "Asia/Shanghai"

      volumeMounts:
        - name: spark-conf
          mountPath: /opt/spark-3.2.1-bin-hadoop2.7/conf/spark-defaults.conf
          subPath: spark-defaults.conf
        - name: hadoop-conf
          mountPath: /opt/spark-3.2.1-bin-hadoop2.7/conf/core-site.xml
          subPath: core-site.xml
        - name: hadoop-conf
          mountPath: /opt/spark-3.2.1-bin-hadoop2.7/conf/yarn-site.xml
          subPath: yarn-site.xml
        - name: hadoop-conf
          mountPath: /opt/spark-3.2.1-bin-hadoop2.7/conf/hive-site.xml
          subPath: hive-site.xml
        - name: hadoop-conf
          mountPath: /opt/spark-3.2.1-bin-hadoop2.7/conf/mapred-site.xml
          subPath: mapred-site.xml
  volumes:
    - name: spark-conf
      configMap:
        name: spark-conf-map
    - name: hadoop-conf
      configMap:
        name: hadoop-conf

---
apiVersion: v1
kind: Service
metadata:
  name: hadoop-spark-client
spec:
  selector:
    app: hadoop-spark-client
  clusterIP: None

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: hadoop-conf
data:
  core-site.xml: |
    <?xml version="1.0"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
    <configuration>
      <property>
            <name>fs.defaultFS</name>
            <value>jfs://myjfs</value>
            <description>NameNode URI</description>
        </property>
      <property>
             <name>fs.jfs.impl</name>
             <value>io.juicefs.JuiceFileSystem</value>
         </property>
         <property>
             <name>fs.AbstractFileSystem.jfs.impl</name>
             <value>io.juicefs.JuiceFS</value>
         </property>
         <property>
             <name>juicefs.meta</name>
             <value>mysql://root:eWJmP7yvpccHCtmVb61Gxl2XLzIrRgmT@(mysql-01:3306)/juicefs2_meta_minio</value>
         </property>
         <property>
             <name>juicefs.cache-size</name>
             <value>1024</value>
         </property>
         <property>
             <name>juicefs.access-log</name>
             <value>/tmp/juicefs.access.log</value>
         </property>
         <property>
             <name>juicefs.cache-dir</name>
             <value>/tmp/data/juicefs</value>
         </property>
    </configuration>


  hive-site.xml: |
    <?xml version="1.0" encoding="UTF-8"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
    <configuration>
        <property>
            <name>hive.metastore.uris</name>
            <value>thrift://hive-metastore-service:9083</value>
        </property>
            <property>
               <name>hive.metastore.warehouse.dir</name>
               <value>/app/hive/warehouse</value>
           </property>
    </configuration>


  yarn-site.xml: |
    <?xml version="1.0"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

    <configuration>
      <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>bigdata-hadoop-yarn-rm</value>
      </property>

      <!-- Bind to all interfaces -->
      <property>
        <name>yarn.resourcemanager.bind-host</name>
        <value>0.0.0.0</value>
      </property>
      <property>
        <name>yarn.nodemanager.bind-host</name>
        <value>0.0.0.0</value>
      </property>
      <property>
        <name>yarn.timeline-service.bind-host</name>
        <value>0.0.0.0</value>
      </property>
      <!-- /Bind to all interfaces -->

      <property>
        <name>yarn.nodemanager.vmem-check-enabled</name>
        <value>false</value>
      </property>

      <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
      </property>

      <property>
        <name>yarn.nodemanager.aux-services.mapreduce_shuffle.class</name>
        <value>org.apache.hadoop.mapred.ShuffleHandler</value>
      </property>

      <property>
        <description>List of directories to store localized files in.</description>
        <name>yarn.nodemanager.local-dirs</name>
        <value>/var/lib/hadoop-yarn/cache/${user.name}/nm-local-dir</value>
      </property>

      <property>
        <description>Where to store container logs.</description>
        <name>yarn.nodemanager.log-dirs</name>
        <value>/var/log/hadoop-yarn/containers</value>
      </property>

      <property>
        <description>Where to aggregate logs to.</description>
        <name>yarn.nodemanager.remote-app-log-dir</name>
        <value>/var/log/hadoop-yarn/apps</value>
      </property>

      <property>
        <name>yarn.application.classpath</name>
        <value>
          /usr/local/hadoop/etc/hadoop,
          /usr/local/hadoop/share/hadoop/common/*,
          /usr/local/hadoop/share/hadoop/common/lib/*,
          /usr/local/hadoop/share/hadoop/hdfs/*,
          /usr/local/hadoop/share/hadoop/hdfs/lib/*,
          /usr/local/hadoop/share/hadoop/mapreduce/*,
          /usr/local/hadoop/share/hadoop/mapreduce/lib/*,
          /usr/local/hadoop/share/hadoop/yarn/*,
          /usr/local/hadoop/share/hadoop/yarn/lib/*
        </value>
      </property>
    </configuration>


  mapred-site.xml: |
    <?xml version="1.0"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

    <configuration>
    	<property>
            <name>mapreduce.framework.name</name>
            <value>yarn</value>
        </property>
        <property>
              <name>mapreduce.map.memory.mb</name>
              <value>5120</value>
        </property>
        <property>
              <name>mapreduce.reduce.memory.mb</name>
              <value>5120</value>
        </property>
    </configuration>



---
apiVersion: v1
kind: ConfigMap
metadata:
  name: spark-conf-map
data:
  spark-defaults.conf: |
    # spark.master                     spark://master:7077
    # spark.eventLog.enabled           true
    # spark.eventLog.dir               hdfs://namenode:8021/directory
    # spark.serializer                 org.apache.spark.serializer.KryoSerializer
    # spark.driver.memory              5g
    # spark.executor.extraJavaOptions  -XX:+PrintGCDetails -Dkey=value -Dnumbers="one two three"
    spark.sql.catalog.spark_catalog    org.apache.iceberg.spark.SparkSessionCatalog


